{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjnuJ19dJV7v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas   1.1.3\n",
      "Sklearn  0.23.2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "import numpy    as np\n",
    "import pandas   as pd\n",
    "import seaborn  as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn  as skl\n",
    "\n",
    "from sklearn import pipeline      # Pipeline\n",
    "from sklearn import preprocessing # OrdinalEncoder, LabelEncoder\n",
    "from sklearn import impute\n",
    "from sklearn import compose\n",
    "from sklearn import model_selection # train_test_split\n",
    "from sklearn import metrics         # accuracy_score, balanced_accuracy_score, plot_confusion_matrix\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram') # Useful for display the pipeline\n",
    "\n",
    "print(\"Pandas  \", pd.__version__)\n",
    "print(\"Sklearn \", skl.__version__) # Try to use 0.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dataset\n",
    "- **CLOUD = True**: Download dataset from Kaggle. Necesary for cloud enviroments like COLAB. **Specify your [kaggle credentials](https://www.kaggle.com/docs/api)**.\n",
    "- **CLOUD = False**: Get the dataset from your local machine. **Specify the data path**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLOUD = False\n",
    "\n",
    "if CLOUD:\n",
    "    import os\n",
    "    os.environ['KAGGLE_USERNAME'] = \"your_kaggle_username\"\n",
    "    os.environ['KAGGLE_KEY']      = \"your_kaggle_api_key\"  # See https://www.kaggle.com/docs/api\n",
    "    !pip install --upgrade kaggle\n",
    "    !kaggle competitions download -c titanic\n",
    "    DATA_PATH = \"./\"\n",
    "\n",
    "else:\n",
    "    DATA_PATH = \"../../datasets/titanic/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAy8TnVPJV8S"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-149-255c37c54b37>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-149-255c37c54b37>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    http://localhost:8888/notebooks/2.%20Exercise%20-%20Tree%20Models%20(Titanic%20dataset).ipynbdf      = pd.read_csv(DATA_PATH + \"train.csv\", index_col='PassengerId')\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "http://localhost:8888/notebooks/2.%20Exercise%20-%20Tree%20Models%20(Titanic%20dataset).ipynbdf      = pd.read_csv(DATA_PATH + \"train.csv\", index_col='PassengerId')\n",
    "df_test = pd.read_csv(DATA_PATH + \"test.csv\",  index_col='PassengerId')\n",
    "\n",
    "print(\"Train DataFrame:\", df.shape)\n",
    "print(\"Test DataFrame: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "Extract the title (Mr, Mrs, ... ) from the \"Name\" column.\n",
    "\n",
    "Tips:\n",
    "- split(',')[1] to get the 2nd part, and remove the surnamename\n",
    "- split('.')[0] to get the 1str part, and remove the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE get_Title_from_Name funtion\n",
    "\n",
    "def get_Title_from_Name(name):\n",
    "    return name[name.find(',') + 2:name.find('.')]\n",
    "\n",
    "df[\"Title\"]      = df['Name'].map(get_Title_from_Name)\n",
    "df_test[\"Title\"] = df_test['Name'].map(get_Title_from_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:\n",
    "Apply the title_dictionary to get a better information about the title. You have to overwrite the Title variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dictionary = {\n",
    "    \"Capt\": \"Officer\",\n",
    "    \"Col\": \"Officer\",\n",
    "    \"Major\": \"Officer\",\n",
    "    \"Jonkheer\": \"Royalty\",\n",
    "    \"Don\": \"Royalty\",\n",
    "    \"Dona\": \"Royalty\",    ('imputer', impute.SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))\n",
    "    \"Sir\" : \"Royalty\",\n",
    "    \"Dr\": \"Officer\",\n",
    "    \"Rev\": \"Officer\",\n",
    "    \"the Countess\":\"Royalty\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Ms\": \"Mrs\",\n",
    "    \"Mr\" : \"Mr\",\n",
    "    \"Mrs\" : \"Mrs\",\n",
    "    \"Miss\" : \"Miss\",\n",
    "    \"Master\" : \"Master\",\n",
    "    \"Lady\" : \"Royalty\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE\n",
    "\n",
    "df[\"Title\"] = df[\"Title\"].map(lambda x: title_dictionary[x])\n",
    "df_test[\"Title\"] = df_test[\"Title\"].map(lambda x: title_dictionary[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise BONUS:\n",
    "Try to extract some information from the feature **Ticket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Ticket', 'Embarked', 'Fare', 'Title', 'Cabin']].loc[df['Ticket'].str.contains('211')]\n",
    "df['T_prx'] = df['Ticket'].map(lambda x: x[:3])\n",
    "df_test['T_prx'] = df_test['Ticket'].map(lambda x: x[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise BONUS:\n",
    "Try to extract some information from the feature **Cabin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Deck'] = df['Cabin'].map(lambda x: x[:1] if not pd.isnull(x) else 'M')\n",
    "df_test['Deck'] = df_test['Cabin'].map(lambda x: x[:1] if not pd.isnull(x) else 'M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "For X data:\n",
    "- We drop Survived because is the target variable\n",
    "- We drop Name because we have extracted the Title: Mr, Mrs, ...\n",
    "- We drop Ticket because it has no information -> see df.Ticket.nunique()\n",
    "- We drop Cabin because it has a lot of missings (77% are missings)\n",
    "\n",
    "Then, we identify **numerical** variables and **categorical** variables,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=[\"Survived\", 'Name', 'Ticket', 'Cabin', 'Deck', 'T_prx']) # X DATA (WILL BE TRAIN+VALID DATA)\n",
    "y = df[\"Survived\"] # 0 = No, 1 = Yes\n",
    "\n",
    "x_test = df_test.drop(columns=['Name', 'Ticket', 'Cabin', 'Deck', 'T_prx']) # # X_TEST DATA (NEW DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical features:\n",
      " ['Pclass', 'SibSp', 'Parch', 'Fare', 'Age']\n",
      "\n",
      "Categorical features:\n",
      " ['Sex', 'Embarked', 'Title']\n"
     ]
    }
   ],
   "source": [
    "cat_vars  = ['Sex', 'Embarked', 'Title']         # x.select_dtypes(include=[object]).columns.values.tolist()\n",
    "num_vars  = ['Pclass', 'SibSp', 'Parch', 'Fare', 'Age'] # x.select_dtypes(exclude=[object]).columns.values.tolist()\n",
    "\n",
    "print(\"\\nNumerical features:\\n\", num_vars)\n",
    "print(\"\\nCategorical features:\\n\", cat_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "Create a **ColumnTransformer for Tree Models**. Remember:\n",
    "- Categorical: Some SimpleImputer -> Some Encoder\n",
    "- Numerical: Some SimpleImputer -> NO Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.sk-top-container {color: black;background-color: white;}div.sk-toggleable {background-color: white;}label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.2em 0.3em;box-sizing: border-box;text-align: center;}div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}div.sk-estimator {font-family: monospace;background-color: #f0f8ff;margin: 0.25em 0.25em;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;}div.sk-estimator:hover {background-color: #d4ebff;}div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;}div.sk-item {z-index: 1;}div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}div.sk-parallel-item:only-child::after {width: 0;}div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0.2em;box-sizing: border-box;padding-bottom: 0.1em;background-color: white;position: relative;}div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}div.sk-label-container {position: relative;z-index: 2;text-align: center;}div.sk-container {display: inline-block;position: relative;}</style><div class=\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a5ec44f6-4f0b-45f2-85a3-9128a148291e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a5ec44f6-4f0b-45f2-85a3-9128a148291e\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                                                 ('scaler', StandardScaler())]),\n",
       "                                 ['Pclass', 'SibSp', 'Parch', 'Fare', 'Age']),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(fill_value='missing',\n",
       "                                                                strategy='constant')),\n",
       "                                                 ('onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 ['Sex', 'Embarked', 'Title'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"fc08eb24-6955-4791-913e-baa4a44520a5\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"fc08eb24-6955-4791-913e-baa4a44520a5\">num</label><div class=\"sk-toggleable__content\"><pre>['Pclass', 'SibSp', 'Parch', 'Fare', 'Age']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5fbfc061-b9ba-41d5-9d7e-4e98178ad9af\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5fbfc061-b9ba-41d5-9d7e-4e98178ad9af\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a24b2118-62aa-4500-b722-7c478ee5af3c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a24b2118-62aa-4500-b722-7c478ee5af3c\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e0e3e827-d653-4f40-bb1b-9faef17f8b22\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e0e3e827-d653-4f40-bb1b-9faef17f8b22\">cat</label><div class=\"sk-toggleable__content\"><pre>['Sex', 'Embarked', 'Title']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"53bdcf7d-dc2a-49d6-b7b5-1ab0be9bc5f8\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"53bdcf7d-dc2a-49d6-b7b5-1ab0be9bc5f8\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value='missing', strategy='constant')</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7a39ffc2-8f30-4870-9172-9bad386c1fea\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7a39ffc2-8f30-4870-9172-9bad386c1fea\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown='ignore')</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                                                 ('scaler', StandardScaler())]),\n",
       "                                 ['Pclass', 'SibSp', 'Parch', 'Fare', 'Age']),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(fill_value='missing',\n",
       "                                                                strategy='constant')),\n",
       "                                                 ('onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 ['Sex', 'Embarked', 'Title'])])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_preprocessing = pipeline.Pipeline(steps=[\n",
    "    ('imputer', impute.SimpleImputer(strategy='mean', add_indicator=False)),\n",
    "    ('scaler', preprocessing.StandardScaler())\n",
    "])\n",
    "\n",
    "cat_preporcessing = pipeline.Pipeline(steps=[\n",
    "    ('imputer', impute.SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "tree_prepro = compose.ColumnTransformer(transformers=[\n",
    "    ('num', num_preprocessing, num_vars),\n",
    "    ('cat', cat_preporcessing, cat_vars),\n",
    "], remainder='drop') # Drop other vars not specified in num_vars or cat_vars\n",
    "\n",
    "tree_prepro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "1. Complete the diccionary with some Tree Models.\n",
    "2. Then we put each model in a Pipeline where:\n",
    "   - first is the prepocessing with the column Transformer\n",
    "   - Then is the Tree model\n",
    "3. Display the fullpipeline of the LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree          import DecisionTreeClassifier\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.ensemble      import ExtraTreesClassifier\n",
    "from sklearn.ensemble      import AdaBoostClassifier\n",
    "from sklearn.ensemble      import GradientBoostingClassifier\n",
    "from sklearn.experimental  import enable_hist_gradient_boosting # Necesary for HistGradientBoostingClassifier\n",
    "from sklearn.ensemble      import HistGradientBoostingClassifier\n",
    "from xgboost               import XGBClassifier\n",
    "from lightgbm              import LGBMClassifier\n",
    "from catboost              import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifiers = {\n",
    "  \"Decision Tree\": DecisionTreeClassifier(),\n",
    "  \"Extra Trees\": ExtraTreesClassifier(),\n",
    "  \"Random Forest\": RandomForestClassifier(),\n",
    "  \"AdaBoost\": AdaBoostClassifier(),\n",
    "  \"Skl GBM\": GradientBoostingClassifier(),\n",
    "  \"Skl HistGBM\": HistGradientBoostingClassifier(),\n",
    "  \"XGBoost\": XGBClassifier(),\n",
    "  \"LightGBM\": LGBMClassifier(),\n",
    "  \"CatBoost\": CatBoostClassifier(verbose=False)\n",
    "}\n",
    "tree_classifiers = {name: pipeline.make_pipeline(tree_prepro, model) for name, model in tree_classifiers.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5:\n",
    "Define a simple split validation strategy with:\n",
    "- 80% for train\n",
    "- 20% for validation\n",
    "- With stratification\n",
    "- random_state=0\n",
    "\n",
    "And train all the models in a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree : 77.65\n",
      "Balanced accuracy of Decision Tree : 75.61\n",
      "Accuracy of Extra Trees : 75.98\n",
      "Balanced accuracy of Extra Trees : 73.43\n",
      "Accuracy of Random Forest : 79.89\n",
      "Balanced accuracy of Random Forest : 77.15\n",
      "Accuracy of AdaBoost : 79.89\n",
      "Balanced accuracy of AdaBoost : 77.42\n",
      "Accuracy of Skl GBM : 84.36\n",
      "Balanced accuracy of Skl GBM : 81.87\n",
      "Accuracy of Skl HistGBM : 80.45\n",
      "Balanced accuracy of Skl HistGBM : 77.88\n",
      "[19:15:19] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGBoost : 81.56\n",
      "Balanced accuracy of XGBoost : 78.79\n",
      "Accuracy of LightGBM : 81.01\n",
      "Balanced accuracy of LightGBM : 78.33\n",
      "Accuracy of CatBoost : 81.01\n",
      "Balanced accuracy of CatBoost : 78.33\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(x, y,\n",
    "    test_size=0.2, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "for model_name, model in tree_classifiers.items():\n",
    "    # CODE HERE\n",
    "    # TRAIN PIPELINE (PREPRO + MODEL) WITH TRAIN DATA\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    # EVAL PIPELINE WITH VAL DATA (SEE ACCURACY AND BALANCED_ACCURACY)\n",
    "    y_pred = model.predict(x_val)\n",
    "    print('Accuracy of', model_name, ':', round(metrics.accuracy_score(y_val, y_pred)*100, 2))\n",
    "    print('Balanced accuracy of', model_name, ':', \n",
    "          round(metrics.balanced_accuracy_score(y_val, y_pred)*100, 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6:\n",
    "Define a 10 Fold cross validation strategy with:\n",
    "- With stratification\n",
    "- shuffle=True\n",
    "- random_state=0\n",
    "\n",
    "And train all the models in a for loop.\n",
    "\n",
    "Tip you can use **[cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html)** for both training and predict with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean of Decision Tree : 0.79 std: 0.0403\n",
      "Balanced accuracy mean of Decision Tree : 0.78 std: 0.037\n",
      "Accuracy mean of Extra Trees : 0.8 std: 0.0322\n",
      "Balanced accuracy mean of Extra Trees : 0.79 std: 0.0314\n",
      "Accuracy mean of Random Forest : 0.81 std: 0.031\n",
      "Balanced accuracy mean of Random Forest : 0.8 std: 0.0328\n",
      "Accuracy mean of AdaBoost : 0.81 std: 0.0487\n",
      "Balanced accuracy mean of AdaBoost : 0.8 std: 0.0496\n",
      "Accuracy mean of Skl GBM : 0.84 std: 0.0286\n",
      "Balanced accuracy mean of Skl GBM : 0.82 std: 0.0323\n",
      "Accuracy mean of Skl HistGBM : 0.82 std: 0.0212\n",
      "Balanced accuracy mean of Skl HistGBM : 0.8 std: 0.0187\n",
      "[19:15:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:15:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:15:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:15:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:15:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:15:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:15:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:15:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:15:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roy/.conda/envs/MachineLearning/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:15:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1607604574104/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy mean of XGBoost : 0.82 std: 0.032\n",
      "Balanced accuracy mean of XGBoost : 0.8 std: 0.0307\n",
      "Accuracy mean of LightGBM : 0.83 std: 0.0312\n",
      "Balanced accuracy mean of LightGBM : 0.81 std: 0.0308\n",
      "Accuracy mean of CatBoost : 0.83 std: 0.0284\n",
      "Balanced accuracy mean of CatBoost : 0.81 std: 0.0288\n"
     ]
    }
   ],
   "source": [
    "skf = model_selection.StratifiedKFold(\n",
    "    n_splits=10, random_state = 0, shuffle=True\n",
    ")\n",
    "\n",
    "for model_name, model in tree_classifiers.items():\n",
    "    acc = []\n",
    "    bacc = []\n",
    "    for train_index, val_index in skf.split(x, y):\n",
    "        X_train, X_val = x.iloc[train_index], x.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        model = model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc.append(metrics.accuracy_score(y_val, y_pred))\n",
    "        bacc.append(metrics.balanced_accuracy_score(y_val, y_pred))\n",
    "    accuracy_mean = np.mean(acc)\n",
    "    balanced_accuracy_mean = np.mean(bacc)\n",
    "    accuracy_std= np.std(acc)\n",
    "    balanced_accuracy_std = np.std(bacc)\n",
    "    print('Accuracy mean of', model_name, ':', round(accuracy_mean, 2), 'std:', \n",
    "          round(accuracy_std, 4))\n",
    "    print('Balanced accuracy mean of', model_name, ':', \n",
    "            round(balanced_accuracy_mean, 2), 'std:', round(balanced_accuracy_std, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "Train **with all data** the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "monuuQhHL7B_"
   },
   "outputs": [],
   "source": [
    "best_model = GradientBoostingClassifier()\n",
    "# Train with all data your best model\n",
    "X = tree_prepro.transform(x)\n",
    "X_test = tree_prepro.transform(x_test)\n",
    "best_model = best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "With your best model, generate the predicitions for test data (x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9\n",
    "\n",
    "Submit to kaggle using the kaggle API. And send us your score. You can try to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "892                 0\n",
       "893                 0\n",
       "894                 0\n",
       "895                 0\n",
       "896                 1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(test_pred, index=x_test.index, columns=[\"Survived\"])\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"//home/roy/kaggle/titanic/submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise BONUS\n",
    "\n",
    "Knowing how to export your models is very important for putting models in production. Try to\n",
    "- Export and Load the ColumTransformer in pickle\n",
    "- Export and Load the ColumTransformer in joblib\n",
    "- Export and load the Pipeline"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Titanic LogReg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "seminar13_optional_practice_trees_titanic.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
